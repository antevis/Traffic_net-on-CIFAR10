{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Self-Driving Car Engineer Nanodegree\n",
    "\n",
    "## Deep Learning\n",
    "\n",
    "## Training Traffic_net on CIFAR10 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "## Step 0: Load The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "import matplotlib.gridspec as gridspec\n",
    "import tensorflow as tf\n",
    "from sklearn.utils import shuffle\n",
    "import time\n",
    "import os\n",
    "\n",
    "import helper as aux\n",
    "import cnn_layers as cnnl\n",
    "import models\n",
    "\n",
    "from keras.datasets import cifar10\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"x:0\", shape=(?, 32, 32, 3), dtype=float32)\n",
      "Tensor(\"dropout/mul:0\", shape=(?, 16, 16, 64), dtype=float32)\n",
      "Tensor(\"dropout_1/mul:0\", shape=(?, 7, 7, 128), dtype=float32)\n",
      "Tensor(\"dropout_2/mul:0\", shape=(?, 3, 3, 192), dtype=float32)\n",
      "Tensor(\"dropout_3/mul:0\", shape=(?, 2048), dtype=float32)\n",
      "Tensor(\"dropout_4/mul:0\", shape=(?, 1024), dtype=float32)\n",
      "Tensor(\"l2_normalize_2:0\", shape=(?, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "### Define your architecture here.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "l_rate = 0.001\n",
    "EPOCHS = 20 #Comes into play if no other early-stopping conditions triggers\n",
    "BATCH_SIZE = 64\n",
    "keep_probability = 0.5\n",
    "\n",
    "output_count = 10\n",
    "\n",
    "ch_count = 3 #image depth\n",
    "l2_beta = 5e-5 #for L2-regularization\n",
    "\n",
    "#Dictionary for conveniently pick later for visualizations\n",
    "tf_names = {'samples': 'x', \n",
    "            'labels' : 'y', \n",
    "            'output' : 'logits', \n",
    "            'acc'    : 'accuracy', \n",
    "            'keep_p' : 'keep_prob'}\n",
    "\n",
    "x = tf.placeholder(tf.float32, (None, 32, 32, ch_count), \n",
    "                   name=tf_names['samples'])\n",
    "\n",
    "y = tf.placeholder(tf.int32, (None), name=tf_names['labels'])\n",
    "one_hot_y = tf.one_hot(y, output_count)\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32, name=tf_names['keep_p'])\n",
    "\n",
    "# Turns out Exponential linear unit (ELU)\n",
    "# gives a slight performance gain\n",
    "logits, weights, model_name = models.traffic_net_v2_full_dropout(x, keep_prob, output_count, activation='elu')\n",
    "\n",
    "#L2 regularization of weights\n",
    "l2_regs = tf.add_n([ tf.nn.l2_loss(v) for v in weights ]) * l2_beta\n",
    "logits = tf.identity(logits, name=tf_names['output'])\n",
    "\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=one_hot_y) + l2_regs\n",
    "loss_operation = tf.reduce_mean(cross_entropy) \n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=l_rate)\n",
    "training_operation = optimizer.minimize(loss_operation)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name=tf_names['acc'])\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "def evaluate(X_data, y_data):\n",
    "    X_data, y_data = shuffle(X_data, y_data)\n",
    "\n",
    "    num_examples = len(X_data)\n",
    "    total_accuracy = 0\n",
    "    sess = tf.get_default_session()\n",
    "    for offset in range(0, num_examples, BATCH_SIZE):\n",
    "        batch_x = X_data[offset:offset+BATCH_SIZE]\n",
    "        batch_y = y_data[offset:offset+BATCH_SIZE]\n",
    "        accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y, keep_prob: 1.0})\n",
    "        total_accuracy += (accuracy * len(batch_x))\n",
    "        \n",
    "    return total_accuracy / num_examples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Train, Validate and Test the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "A validation set can be used to assess how well the model is performing. A low accuracy on the training and validation\n",
    "sets imply underfitting. A high accuracy on the training set but low accuracy on the validation set implies overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "Untaring file...\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "# y_train.shape is 2d, (50000, 1). While Keras is smart enough to handle this\n",
    "# it's a good idea to flatten the array.\n",
    "y_train = y_train.reshape(-1)\n",
    "y_test = y_test.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train,\n",
    "                                                  y_train, test_size=.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training..\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-4ea26d09c03e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m                 sess.run(training_operation, \n\u001b[1;32m     39\u001b[0m                          feed_dict={x: batch_x, y: batch_y, \n\u001b[0;32m---> 40\u001b[0;31m                                     keep_prob: keep_probability})\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ivan/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ivan/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ivan/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/ivan/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ivan/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_accuracy = 0.0\n",
    "\n",
    "run = 0\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "print('Training..')\n",
    "\n",
    "while run < 1 and best_accuracy < 1.0:\n",
    "\n",
    "    save_file = './{}_{}.ckpt'.format(model_name, run)\n",
    "\n",
    "    validation_accuracy = 0.0\n",
    "    valid_acc_array = np.array([])\n",
    "    train_acc_array = np.array([])\n",
    "    no_improvement_count = 0\n",
    "    \n",
    "    sigma = 0\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        num_examples = len(X_train)\n",
    "\n",
    "        i = 0\n",
    "\n",
    "        #Defining early stopping.\n",
    "        noit = 15 #Allowed number of iteration without improvement\n",
    "        nois = 0.0015 #St dev of noit^ last validation accuracies\n",
    "        while no_improvement_count < noit \\\n",
    "        and (len(valid_acc_array) < 5 or sigma > nois) \\\n",
    "        and validation_accuracy < 1.0 and i < EPOCHS:\n",
    "            #feature_range = tqdm(range(0, num_examples, BATCH_SIZE))\n",
    "            feature_range = range(0, num_examples, BATCH_SIZE)\n",
    "            \n",
    "            X_train, y_train = shuffle(X_train, y_train)\n",
    "\n",
    "            for offset in feature_range:\n",
    "                end = offset + BATCH_SIZE\n",
    "                batch_x, batch_y = X_train[offset:end], y_train[offset:end]\n",
    "                sess.run(training_operation, \n",
    "                         feed_dict={x: batch_x, y: batch_y, \n",
    "                                    keep_prob: keep_probability})\n",
    "\n",
    "            train_acc = evaluate(X_train, y_train)\n",
    "            validation_accuracy = evaluate(X_valid, y_valid)\n",
    "            train_acc_array = np.append(train_acc_array, train_acc)\n",
    "            valid_acc_array = np.append(valid_acc_array, validation_accuracy)\n",
    "            sigma = np.std(valid_acc_array[-noit:])\n",
    "\n",
    "            if validation_accuracy > best_accuracy:\n",
    "                best_accuracy = validation_accuracy\n",
    "                saver.save(sess, save_file)\n",
    "                no_improvement_count = 0\n",
    "            elif no_improvement_count > 0 and len(valid_acc_array) > 0 \\\n",
    "            and validation_accuracy > valid_acc_array[-1]:\n",
    "                # That's a bit controversial but allows net to recover \n",
    "                # after significant accuracy drops, when chosen tolerance \n",
    "                # may not be enough to recover\n",
    "                no_improvement_count -= 1\n",
    "            else:\n",
    "                no_improvement_count += 1\n",
    "\n",
    "            print(\"RUN {} | EPOCH | {} | train_acc = {:.4f} | val_acc = {:.4f} | valid_sigma 15: {:.4f}\"\n",
    "                  .format(run, i, train_acc, validation_accuracy, sigma))\n",
    "\n",
    "            i += 1\n",
    "\n",
    "        run += 1\n",
    "\n",
    "aux.timing_stats_since(start=start, message='Training Complete. Global Best valid accuracy: {}\\n'.format(best_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "save_file = './{}_0.ckpt'.format(model_name)\n",
    "    \n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    #sess.run(tf.global_variables_initializer()) #Lecture says no need to call this.\n",
    "    saver.restore(sess, save_file)\n",
    "    \n",
    "    test_accuracy = evaluate(X_test, y_test)\n",
    "    print(\"Test Accuracy = {:.3f}\".format(test_accuracy))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
