{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Self-Driving Car Engineer Nanodegree\n",
    "\n",
    "## Deep Learning\n",
    "\n",
    "## Training Traffic_net on CIFAR10 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "## Step 0: Load The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "import matplotlib.gridspec as gridspec\n",
    "import tensorflow as tf\n",
    "from sklearn.utils import shuffle\n",
    "import time\n",
    "import os\n",
    "\n",
    "import helper as aux\n",
    "import cnn_layers as cnnl\n",
    "import models\n",
    "\n",
    "from keras.datasets import cifar10\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"x:0\", shape=(?, 32, 32, 3), dtype=float32)\n",
      "Tensor(\"dropout/mul:0\", shape=(?, 16, 16, 64), dtype=float32)\n",
      "Tensor(\"dropout_1/mul:0\", shape=(?, 7, 7, 128), dtype=float32)\n",
      "Tensor(\"dropout_2/mul:0\", shape=(?, 3, 3, 192), dtype=float32)\n",
      "Tensor(\"dropout_3/mul:0\", shape=(?, 2048), dtype=float32)\n",
      "Tensor(\"dropout_4/mul:0\", shape=(?, 1024), dtype=float32)\n",
      "Tensor(\"l2_normalize_2:0\", shape=(?, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "### Define your architecture here.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "l_rate = 0.001\n",
    "EPOCHS = 20 #Comes into play if no other early-stopping conditions triggers\n",
    "BATCH_SIZE = 64\n",
    "keep_probability = 0.5\n",
    "\n",
    "output_count = 10\n",
    "\n",
    "ch_count = 3 #image depth\n",
    "l2_beta = 5e-5 #for L2-regularization\n",
    "\n",
    "#Dictionary for conveniently pick later for visualizations\n",
    "tf_names = {'samples': 'x', \n",
    "            'labels' : 'y', \n",
    "            'output' : 'logits', \n",
    "            'acc'    : 'accuracy', \n",
    "            'keep_p' : 'keep_prob'}\n",
    "\n",
    "x = tf.placeholder(tf.float32, (None, 32, 32, ch_count), \n",
    "                   name=tf_names['samples'])\n",
    "\n",
    "y = tf.placeholder(tf.int32, (None), name=tf_names['labels'])\n",
    "one_hot_y = tf.one_hot(y, output_count)\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32, name=tf_names['keep_p'])\n",
    "\n",
    "# Turns out Exponential linear unit (ELU)\n",
    "# gives a slight performance gain\n",
    "logits, weights, model_name = models.traffic_net_v2_full_dropout(x, keep_prob, output_count, activation='elu')\n",
    "\n",
    "#L2 regularization of weights\n",
    "l2_regs = tf.add_n([ tf.nn.l2_loss(v) for v in weights ]) * l2_beta\n",
    "logits = tf.identity(logits, name=tf_names['output'])\n",
    "\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=one_hot_y) + l2_regs\n",
    "loss_operation = tf.reduce_mean(cross_entropy) \n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=l_rate)\n",
    "training_operation = optimizer.minimize(loss_operation)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name=tf_names['acc'])\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "def evaluate(X_data, y_data):\n",
    "    X_data, y_data = shuffle(X_data, y_data)\n",
    "\n",
    "    num_examples = len(X_data)\n",
    "    total_accuracy = 0\n",
    "    sess = tf.get_default_session()\n",
    "    for offset in range(0, num_examples, BATCH_SIZE):\n",
    "        batch_x = X_data[offset:offset+BATCH_SIZE]\n",
    "        batch_y = y_data[offset:offset+BATCH_SIZE]\n",
    "        accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y, keep_prob: 1.0})\n",
    "        total_accuracy += (accuracy * len(batch_x))\n",
    "        \n",
    "    return total_accuracy / num_examples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Train, Validate and Test the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "A validation set can be used to assess how well the model is performing. A low accuracy on the training and validation\n",
    "sets imply underfitting. A high accuracy on the training set but low accuracy on the validation set implies overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "Untaring file...\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "# y_train.shape is 2d, (50000, 1). While Keras is smart enough to handle this\n",
    "# it's a good idea to flatten the array.\n",
    "y_train = y_train.reshape(-1)\n",
    "y_test = y_test.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train,\n",
    "                                                  y_train, test_size=.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training..\n",
      "RUN 0 | EPOCH | 0 | train_acc = 0.4125 | val_acc = 0.4068 | valid_sigma 15: 0.0000\n",
      "RUN 0 | EPOCH | 1 | train_acc = 0.5121 | val_acc = 0.5040 | valid_sigma 15: 0.0486\n",
      "RUN 0 | EPOCH | 2 | train_acc = 0.5616 | val_acc = 0.5531 | valid_sigma 15: 0.0608\n",
      "RUN 0 | EPOCH | 3 | train_acc = 0.5801 | val_acc = 0.5689 | valid_sigma 15: 0.0632\n",
      "RUN 0 | EPOCH | 4 | train_acc = 0.5841 | val_acc = 0.5679 | valid_sigma 15: 0.0614\n",
      "RUN 0 | EPOCH | 5 | train_acc = 0.6017 | val_acc = 0.5782 | valid_sigma 15: 0.0601\n",
      "RUN 0 | EPOCH | 6 | train_acc = 0.6263 | val_acc = 0.6077 | valid_sigma 15: 0.0619\n",
      "RUN 0 | EPOCH | 7 | train_acc = 0.6424 | val_acc = 0.6203 | valid_sigma 15: 0.0636\n",
      "RUN 0 | EPOCH | 8 | train_acc = 0.6495 | val_acc = 0.6236 | valid_sigma 15: 0.0642\n",
      "RUN 0 | EPOCH | 9 | train_acc = 0.6927 | val_acc = 0.6631 | valid_sigma 15: 0.0684\n",
      "RUN 0 | EPOCH | 10 | train_acc = 0.6974 | val_acc = 0.6704 | valid_sigma 15: 0.0714\n",
      "RUN 0 | EPOCH | 11 | train_acc = 0.7067 | val_acc = 0.6751 | valid_sigma 15: 0.0734\n",
      "RUN 0 | EPOCH | 12 | train_acc = 0.7362 | val_acc = 0.6997 | valid_sigma 15: 0.0767\n",
      "RUN 0 | EPOCH | 13 | train_acc = 0.7296 | val_acc = 0.6935 | valid_sigma 15: 0.0781\n",
      "RUN 0 | EPOCH | 14 | train_acc = 0.7346 | val_acc = 0.6933 | valid_sigma 15: 0.0788\n",
      "RUN 0 | EPOCH | 15 | train_acc = 0.7413 | val_acc = 0.7017 | valid_sigma 15: 0.0608\n",
      "RUN 0 | EPOCH | 16 | train_acc = 0.7381 | val_acc = 0.7008 | valid_sigma 15: 0.0534\n",
      "RUN 0 | EPOCH | 17 | train_acc = 0.7380 | val_acc = 0.7050 | valid_sigma 15: 0.0500\n",
      "RUN 0 | EPOCH | 18 | train_acc = 0.7610 | val_acc = 0.7186 | valid_sigma 15: 0.0475\n",
      "RUN 0 | EPOCH | 19 | train_acc = 0.7479 | val_acc = 0.7083 | valid_sigma 15: 0.0416\n",
      "Training Complete. Global Best valid accuracy: 0.7186\n",
      "Time elapsed: 12 m 1.614 s\n"
     ]
    }
   ],
   "source": [
    "best_accuracy = 0.0\n",
    "\n",
    "run = 0\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "print('Training..')\n",
    "\n",
    "while run < 1 and best_accuracy < 1.0:\n",
    "\n",
    "    save_file = './{}_{}.ckpt'.format(model_name, run)\n",
    "\n",
    "    validation_accuracy = 0.0\n",
    "    valid_acc_array = np.array([])\n",
    "    train_acc_array = np.array([])\n",
    "    no_improvement_count = 0\n",
    "    \n",
    "    sigma = 0\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        num_examples = len(X_train)\n",
    "\n",
    "        i = 0\n",
    "\n",
    "        #Defining early stopping.\n",
    "        noit = 15 #Allowed number of iteration without improvement\n",
    "        nois = 0.0015 #St dev of noit^ last validation accuracies\n",
    "        while no_improvement_count < noit \\\n",
    "        and (len(valid_acc_array) < 5 or sigma > nois) \\\n",
    "        and validation_accuracy < 1.0 and i < EPOCHS:\n",
    "            #feature_range = tqdm(range(0, num_examples, BATCH_SIZE))\n",
    "            feature_range = range(0, num_examples, BATCH_SIZE)\n",
    "            \n",
    "            X_train, y_train = shuffle(X_train, y_train)\n",
    "\n",
    "            for offset in feature_range:\n",
    "                end = offset + BATCH_SIZE\n",
    "                batch_x, batch_y = X_train[offset:end], y_train[offset:end]\n",
    "                sess.run(training_operation, \n",
    "                         feed_dict={x: batch_x, y: batch_y, \n",
    "                                    keep_prob: keep_probability})\n",
    "\n",
    "            train_acc = evaluate(X_train, y_train)\n",
    "            validation_accuracy = evaluate(X_valid, y_valid)\n",
    "            train_acc_array = np.append(train_acc_array, train_acc)\n",
    "            valid_acc_array = np.append(valid_acc_array, validation_accuracy)\n",
    "            sigma = np.std(valid_acc_array[-noit:])\n",
    "\n",
    "            if validation_accuracy > best_accuracy:\n",
    "                best_accuracy = validation_accuracy\n",
    "                saver.save(sess, save_file)\n",
    "                no_improvement_count = 0\n",
    "            elif no_improvement_count > 0 and len(valid_acc_array) > 0 \\\n",
    "            and validation_accuracy > valid_acc_array[-1]:\n",
    "                # That's a bit controversial but allows net to recover \n",
    "                # after significant accuracy drops, when chosen tolerance \n",
    "                # may not be enough to recover\n",
    "                no_improvement_count -= 1\n",
    "            else:\n",
    "                no_improvement_count += 1\n",
    "\n",
    "            print(\"RUN {} | EPOCH | {} | train_acc = {:.4f} | val_acc = {:.4f} | valid_sigma 15: {:.4f}\"\n",
    "                  .format(run, i, train_acc, validation_accuracy, sigma))\n",
    "\n",
    "            i += 1\n",
    "\n",
    "        run += 1\n",
    "\n",
    "aux.timing_stats_since(start=start, message='Training Complete. Global Best valid accuracy: {}\\n'.format(best_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy = 0.716\n"
     ]
    }
   ],
   "source": [
    "save_file = './{}_0.ckpt'.format(model_name)\n",
    "    \n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    #sess.run(tf.global_variables_initializer()) #Lecture says no need to call this.\n",
    "    saver.restore(sess, save_file)\n",
    "    \n",
    "    test_accuracy = evaluate(X_test, y_test)\n",
    "    print(\"Test Accuracy = {:.3f}\".format(test_accuracy))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
